# Main configuration file for Vision PyTorch Lightning Framework
# Uses Hydra for configuration management

defaults:
  - _self_
  - model: resnet
  - data: cifar10
  - trainer: default
  - augmentation: standard
  - callbacks: default
  - logger: mlflow

# Experiment settings
experiment:
  name: "vision_experiment"
  seed: 42
  description: "Vision model training experiment"
  tags:
    - vision
    - pytorch_lightning

# Hydra configuration
hydra:
  run:
    dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}
  job:
    name: ${experiment.name}
    chdir: true
  job_logging:
    formatters:
      simple:
        format: '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'

# Paths configuration
paths:
  root_dir: ${oc.env:PROJECT_ROOT,.}
  data_dir: ${paths.root_dir}/data
  log_dir: ${paths.root_dir}/logs
  output_dir: ${hydra:runtime.output_dir}
  work_dir: ${hydra:runtime.cwd}

# Training hyperparameters (can be overridden by model configs)
training:
  batch_size: 32
  learning_rate: 0.001
  weight_decay: 0.0001
  optimizer: "adamw"
  scheduler:
    type: "cosine"
    T_max: 100
    eta_min: 1e-6

# Monitoring and logging
monitoring:
  log_every_n_steps: 10
  val_check_interval: 1.0
  save_top_k: 3
  monitor_metric: "val/loss"
  monitor_mode: "min"

# Optimization settings
optimization:
  gradient_clip_val: 1.0
  gradient_clip_algorithm: "norm"
  accumulate_grad_batches: 1
  automatic_optimization: true

# Hardware settings
hardware:
  accelerator: "auto"  # auto, gpu, tpu, cpu
  devices: "auto"
  precision: 16  # 16 for mixed precision, 32 for full precision
  num_workers: 4
  pin_memory: true
  persistent_workers: true

# Debug settings
debug:
  fast_dev_run: false
  overfit_batches: 0
  profiler: null  # simple, advanced, pytorch
  detect_anomaly: false
  track_grad_norm: -1

# Reproducibility
reproducibility:
  deterministic: false
  benchmark: true